{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d57028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Horizon: 1 hours ===\n",
      "Converting object columns to numeric: ['wd_E_lag1', 'wd_ENE_lag1', 'wd_ESE_lag1', 'wd_N_lag1', 'wd_NE_lag1', 'wd_NNE_lag1', 'wd_NNW_lag1', 'wd_NW_lag1', 'wd_S_lag1', 'wd_SE_lag1', 'wd_SSE_lag1', 'wd_SSW_lag1', 'wd_SW_lag1', 'wd_W_lag1', 'wd_WNW_lag1', 'wd_WSW_lag1', 'wd_E_lag2', 'wd_ENE_lag2', 'wd_ESE_lag2', 'wd_N_lag2', 'wd_NE_lag2', 'wd_NNE_lag2', 'wd_NNW_lag2', 'wd_NW_lag2', 'wd_S_lag2', 'wd_SE_lag2', 'wd_SSE_lag2', 'wd_SSW_lag2', 'wd_SW_lag2', 'wd_W_lag2', 'wd_WNW_lag2', 'wd_WSW_lag2', 'wd_E_lag3', 'wd_ENE_lag3', 'wd_ESE_lag3', 'wd_N_lag3', 'wd_NE_lag3', 'wd_NNE_lag3', 'wd_NNW_lag3', 'wd_NW_lag3', 'wd_S_lag3', 'wd_SE_lag3', 'wd_SSE_lag3', 'wd_SSW_lag3', 'wd_SW_lag3', 'wd_W_lag3', 'wd_WNW_lag3', 'wd_WSW_lag3']\n",
      "Naive RMSE: 30.617581499440146\n",
      " Training: Linear\n",
      "  Linear RMSE: 17.4797\n",
      " Training: RandomForest\n",
      "  RandomForest RMSE: 18.1745\n",
      " Training: XGBoost\n",
      "  XGBoost RMSE: 19.3884\n",
      "\n",
      "=== Horizon: 3 hours ===\n",
      "Converting object columns to numeric: ['wd_E_lag1', 'wd_ENE_lag1', 'wd_ESE_lag1', 'wd_N_lag1', 'wd_NE_lag1', 'wd_NNE_lag1', 'wd_NNW_lag1', 'wd_NW_lag1', 'wd_S_lag1', 'wd_SE_lag1', 'wd_SSE_lag1', 'wd_SSW_lag1', 'wd_SW_lag1', 'wd_W_lag1', 'wd_WNW_lag1', 'wd_WSW_lag1', 'wd_E_lag2', 'wd_ENE_lag2', 'wd_ESE_lag2', 'wd_N_lag2', 'wd_NE_lag2', 'wd_NNE_lag2', 'wd_NNW_lag2', 'wd_NW_lag2', 'wd_S_lag2', 'wd_SE_lag2', 'wd_SSE_lag2', 'wd_SSW_lag2', 'wd_SW_lag2', 'wd_W_lag2', 'wd_WNW_lag2', 'wd_WSW_lag2', 'wd_E_lag3', 'wd_ENE_lag3', 'wd_ESE_lag3', 'wd_N_lag3', 'wd_NE_lag3', 'wd_NNE_lag3', 'wd_NNW_lag3', 'wd_NW_lag3', 'wd_S_lag3', 'wd_SE_lag3', 'wd_SSE_lag3', 'wd_SSW_lag3', 'wd_SW_lag3', 'wd_W_lag3', 'wd_WNW_lag3', 'wd_WSW_lag3']\n",
      "Naive RMSE: 45.46500681820124\n",
      " Training: Linear\n",
      "  Linear RMSE: 36.3460\n",
      " Training: RandomForest\n",
      "  RandomForest RMSE: 36.5179\n",
      " Training: XGBoost\n",
      "  XGBoost RMSE: 39.5003\n",
      "\n",
      "=== Horizon: 6 hours ===\n",
      "Converting object columns to numeric: ['wd_E_lag1', 'wd_ENE_lag1', 'wd_ESE_lag1', 'wd_N_lag1', 'wd_NE_lag1', 'wd_NNE_lag1', 'wd_NNW_lag1', 'wd_NW_lag1', 'wd_S_lag1', 'wd_SE_lag1', 'wd_SSE_lag1', 'wd_SSW_lag1', 'wd_SW_lag1', 'wd_W_lag1', 'wd_WNW_lag1', 'wd_WSW_lag1', 'wd_E_lag2', 'wd_ENE_lag2', 'wd_ESE_lag2', 'wd_N_lag2', 'wd_NE_lag2', 'wd_NNE_lag2', 'wd_NNW_lag2', 'wd_NW_lag2', 'wd_S_lag2', 'wd_SE_lag2', 'wd_SSE_lag2', 'wd_SSW_lag2', 'wd_SW_lag2', 'wd_W_lag2', 'wd_WNW_lag2', 'wd_WSW_lag2', 'wd_E_lag3', 'wd_ENE_lag3', 'wd_ESE_lag3', 'wd_N_lag3', 'wd_NE_lag3', 'wd_NNE_lag3', 'wd_NNW_lag3', 'wd_NW_lag3', 'wd_S_lag3', 'wd_SE_lag3', 'wd_SSE_lag3', 'wd_SSW_lag3', 'wd_SW_lag3', 'wd_W_lag3', 'wd_WNW_lag3', 'wd_WSW_lag3']\n",
      "Naive RMSE: 60.07334475378995\n",
      " Training: Linear\n",
      "  Linear RMSE: 50.5561\n",
      " Training: RandomForest\n",
      "  RandomForest RMSE: 51.2466\n",
      " Training: XGBoost\n",
      "  XGBoost RMSE: 53.9621\n",
      "\n",
      "=== Horizon: 12 hours ===\n",
      "Converting object columns to numeric: ['wd_E_lag1', 'wd_ENE_lag1', 'wd_ESE_lag1', 'wd_N_lag1', 'wd_NE_lag1', 'wd_NNE_lag1', 'wd_NNW_lag1', 'wd_NW_lag1', 'wd_S_lag1', 'wd_SE_lag1', 'wd_SSE_lag1', 'wd_SSW_lag1', 'wd_SW_lag1', 'wd_W_lag1', 'wd_WNW_lag1', 'wd_WSW_lag1', 'wd_E_lag2', 'wd_ENE_lag2', 'wd_ESE_lag2', 'wd_N_lag2', 'wd_NE_lag2', 'wd_NNE_lag2', 'wd_NNW_lag2', 'wd_NW_lag2', 'wd_S_lag2', 'wd_SE_lag2', 'wd_SSE_lag2', 'wd_SSW_lag2', 'wd_SW_lag2', 'wd_W_lag2', 'wd_WNW_lag2', 'wd_WSW_lag2', 'wd_E_lag3', 'wd_ENE_lag3', 'wd_ESE_lag3', 'wd_N_lag3', 'wd_NE_lag3', 'wd_NNE_lag3', 'wd_NNW_lag3', 'wd_NW_lag3', 'wd_S_lag3', 'wd_SE_lag3', 'wd_SSE_lag3', 'wd_SSW_lag3', 'wd_SW_lag3', 'wd_W_lag3', 'wd_WNW_lag3', 'wd_WSW_lag3']\n",
      "Naive RMSE: 75.4470118859985\n",
      " Training: Linear\n",
      "  Linear RMSE: 62.8072\n",
      " Training: RandomForest\n",
      "  RandomForest RMSE: 63.6896\n",
      " Training: XGBoost\n",
      "  XGBoost RMSE: 70.3449\n",
      "\n",
      "=== Horizon: 24 hours ===\n",
      "Converting object columns to numeric: ['wd_E_lag1', 'wd_ENE_lag1', 'wd_ESE_lag1', 'wd_N_lag1', 'wd_NE_lag1', 'wd_NNE_lag1', 'wd_NNW_lag1', 'wd_NW_lag1', 'wd_S_lag1', 'wd_SE_lag1', 'wd_SSE_lag1', 'wd_SSW_lag1', 'wd_SW_lag1', 'wd_W_lag1', 'wd_WNW_lag1', 'wd_WSW_lag1', 'wd_E_lag2', 'wd_ENE_lag2', 'wd_ESE_lag2', 'wd_N_lag2', 'wd_NE_lag2', 'wd_NNE_lag2', 'wd_NNW_lag2', 'wd_NW_lag2', 'wd_S_lag2', 'wd_SE_lag2', 'wd_SSE_lag2', 'wd_SSW_lag2', 'wd_SW_lag2', 'wd_W_lag2', 'wd_WNW_lag2', 'wd_WSW_lag2', 'wd_E_lag3', 'wd_ENE_lag3', 'wd_ESE_lag3', 'wd_N_lag3', 'wd_NE_lag3', 'wd_NNE_lag3', 'wd_NNW_lag3', 'wd_NW_lag3', 'wd_S_lag3', 'wd_SE_lag3', 'wd_SSE_lag3', 'wd_SSW_lag3', 'wd_SW_lag3', 'wd_W_lag3', 'wd_WNW_lag3', 'wd_WSW_lag3']\n",
      "Naive RMSE: 90.92859894219507\n",
      " Training: Linear\n",
      "  Linear RMSE: 72.2662\n",
      " Training: RandomForest\n",
      "  RandomForest RMSE: 73.4038\n",
      " Training: XGBoost\n",
      "  XGBoost RMSE: 79.6635\n",
      "Done. Results saved to: results_multi_horizon\n"
     ]
    }
   ],
   "source": [
    "# Run in your environment (Python >=3.8). Requires: pandas, numpy, scikit-learn, xgboost, joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ------------- config -------------\n",
    "DATA_PATH = \"PRSA_Aotizhongxin_cleaned.csv\"   # replace with your file path\n",
    "TARGET = \"PM2.5\"\n",
    "LAGS = 3\n",
    "HORIZONS = [1, 3, 6, 12, 24]\n",
    "TRAIN_FRAC = 0.8\n",
    "RESULT_DIR = \"results_multi_horizon\"\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "# -----------------------------------\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=True, index_col=0)  # index should be datetime\n",
    "\n",
    "def make_supervised(df, target_col, lags=3, horizon=1):\n",
    "    df_sup = df.copy()\n",
    "    # create lag features for all columns\n",
    "    for lag in range(1, lags + 1):\n",
    "        df_sup = df_sup.assign(**{f\"{c}_lag{lag}\": df[c].shift(lag) for c in df.columns})\n",
    "\n",
    "    # target shifted up by horizon (predict future)\n",
    "    df_sup[f\"y_h{horizon}\"] = df_sup[target_col].shift(-horizon)\n",
    "    df_sup = df_sup.dropna()\n",
    "\n",
    "    X = df_sup.drop(columns=[f\"y_h{horizon}\"])\n",
    "    y = df_sup[f\"y_h{horizon}\"]\n",
    "    return X, y\n",
    "\n",
    "def train_test_split_time(X, y, train_frac=0.8):\n",
    "    n = len(X)\n",
    "    split = int(n * train_frac)\n",
    "    X_train, X_test = X.iloc[:split], X.iloc[split:]\n",
    "    y_train, y_test = y.iloc[:split], y.iloc[split:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def ensure_numeric(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Convert object dtypes to numeric/boolean. Coerce invalid -> NaN, then fillna(0).\n",
    "    Returns numeric X_train, X_test (floats).\n",
    "    \"\"\"\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    # find object columns\n",
    "    obj_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    if obj_cols:\n",
    "        print(\"Converting object columns to numeric:\", obj_cols)\n",
    "    for c in obj_cols:\n",
    "        # try numeric conversion\n",
    "        X_train[c] = pd.to_numeric(X_train[c], errors='coerce')\n",
    "        X_test[c] = pd.to_numeric(X_test[c], errors='coerce')\n",
    "\n",
    "        # if still object (unlikely), try bool mapping\n",
    "        if X_train[c].dtype == 'object' or X_test[c].dtype == 'object':\n",
    "            X_train[c] = X_train[c].map({'True':1, 'False':0, True:1, False:0}).astype('float')\n",
    "            X_test[c] = X_test[c].map({'True':1, 'False':0, True:1, False:0}).astype('float')\n",
    "\n",
    "    # After conversion, fill NaNs with 0 (safe for one-hot and numeric lag features)\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "    # finally cast to float (XGBoost and sklearn expect numeric arrays)\n",
    "    X_train = X_train.astype(float)\n",
    "    X_test = X_test.astype(float)\n",
    "    return X_train, X_test\n",
    "\n",
    "models = {\n",
    "    \"Naive\": None,  # handled separately\n",
    "    \"Linear\": make_pipeline(StandardScaler(), LinearRegression()),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": xgb.XGBRegressor(n_estimators=200, random_state=42, n_jobs=4, verbosity=0)\n",
    "}\n",
    "\n",
    "results = []\n",
    "per_hour_errors = {}  # store arrays for paired tests\n",
    "\n",
    "for h in HORIZONS:\n",
    "    print(f\"\\n=== Horizon: {h} hours ===\")\n",
    "    X, y = make_supervised(df, TARGET, lags=LAGS, horizon=h)\n",
    "    X_train, X_test, y_train, y_test = train_test_split_time(X, y, TRAIN_FRAC)\n",
    "\n",
    "    # Ensure numeric dtypes before any model uses X (including naive baseline)\n",
    "    X_train_num, X_test_num = ensure_numeric(X_train, X_test)\n",
    "\n",
    "    # Naive baseline: PM2.5_lag1 (ensure it's numeric)\n",
    "    naive_pred = X_test_num[f\"{TARGET}_lag1\"].values\n",
    "    naive_rmse = mean_squared_error(y_test, naive_pred) ** 0.5\n",
    "    print(\"Naive RMSE:\", naive_rmse)\n",
    "\n",
    "    results.append({\"horizon\": h, \"model\": \"Naive\", \"rmse\": float(naive_rmse)})\n",
    "    per_hour_errors[(h, \"Naive\")] = (y_test.values - naive_pred)\n",
    "\n",
    "    for mname, m in models.items():\n",
    "        if mname == \"Naive\":\n",
    "            continue\n",
    "        print(\" Training:\", mname)\n",
    "\n",
    "        # For models that include preprocessing (e.g., Linear pipeline), use original Xframes but numeric\n",
    "        # For RandomForest and XGBoost we pass numeric arrays\n",
    "        if mname == \"Linear\":\n",
    "            # Linear pipeline expects DataFrame-like input; using the numeric DataFrames is fine\n",
    "            m.fit(X_train_num, y_train)\n",
    "            yhat = m.predict(X_test_num)\n",
    "        else:\n",
    "            # RandomForest and XGBoost also accept DataFrames or arrays\n",
    "            m.fit(X_train_num, y_train)\n",
    "            yhat = m.predict(X_test_num)\n",
    "\n",
    "        rmse = mean_squared_error(y_test, yhat) ** 0.5\n",
    "        print(f\"  {mname} RMSE: {rmse:.4f}\")\n",
    "\n",
    "        results.append({\"horizon\": h, \"model\": mname, \"rmse\": float(rmse)})\n",
    "        per_hour_errors[(h, mname)] = (y_test.values - yhat)\n",
    "\n",
    "        # optional: save model\n",
    "        joblib.dump(m, os.path.join(RESULT_DIR, f\"{mname}_h{h}.joblib\"))\n",
    "\n",
    "# save summary\n",
    "pd.DataFrame(results).pivot(index=\"model\", columns=\"horizon\", values=\"rmse\") \\\n",
    "    .to_csv(os.path.join(RESULT_DIR, \"rmse_table.csv\"))\n",
    "\n",
    "# save per-hour errors (for statistical tests later)\n",
    "joblib.dump(per_hour_errors, os.path.join(RESULT_DIR, \"per_hour_errors.joblib\"))\n",
    "\n",
    "print(\"Done. Results saved to:\", RESULT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6eb9301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE table (models rows, horizons columns):\n",
      "                      1          3          6         12         24\n",
      "model                                                              \n",
      "Linear        17.479715  36.345979  50.556133  62.807161  72.266227\n",
      "Naive         30.617581  45.465007  60.073345  75.447012  90.928599\n",
      "RandomForest  18.174493  36.517867  51.246618  63.689615  73.403836\n",
      "XGBoost       19.388413  39.500271  53.962063  70.344928  79.663455\n",
      "\n",
      "Processing horizon 1h\n",
      "H1h paired t-test Naive vs RF: t=1.007, p=3.138e-01; Wilcoxon: stat=10251246.0, p=1.962e-33\n",
      "\n",
      "Processing horizon 3h\n",
      "H3h paired t-test Naive vs RF: t=3.623, p=2.937e-04; Wilcoxon: stat=8833302.5, p=1.267e-92\n",
      "\n",
      "Processing horizon 6h\n",
      "H6h paired t-test Naive vs RF: t=3.027, p=2.480e-03; Wilcoxon: stat=8219865.5, p=1.861e-127\n",
      "\n",
      "Processing horizon 12h\n",
      "H12h paired t-test Naive vs RF: t=2.994, p=2.761e-03; Wilcoxon: stat=8198355.0, p=1.268e-128\n",
      "\n",
      "Processing horizon 24h\n",
      "H24h paired t-test Naive vs RF: t=10.089, p=8.972e-24; Wilcoxon: stat=7258013.5, p=3.505e-193\n",
      "\n",
      "All plots saved to: results_multi_horizon/plots\n"
     ]
    }
   ],
   "source": [
    "# save as analysis_and_plots.py and run it\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 150})\n",
    "\n",
    "RESULT_DIR = \"results_multi_horizon\"\n",
    "PLOTS_DIR = os.path.join(RESULT_DIR, \"plots\")\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# 1) load rmse table and per-hour errors\n",
    "rmse_table = pd.read_csv(os.path.join(RESULT_DIR, \"rmse_table.csv\"), index_col=0)\n",
    "per_hour_errors = joblib.load(os.path.join(RESULT_DIR, \"per_hour_errors.joblib\"))\n",
    "\n",
    "# show RMSE table\n",
    "print(\"\\nRMSE table (models rows, horizons columns):\")\n",
    "print(rmse_table)\n",
    "\n",
    "# 2) RMSE vs horizon plot (line)\n",
    "plt.figure()\n",
    "for model in rmse_table.index:\n",
    "    plt.plot(rmse_table.columns.astype(int), rmse_table.loc[model].values, marker='o', label=model)\n",
    "plt.xlabel(\"Horizon (hours)\")\n",
    "plt.ylabel(\"RMSE (µg/m³)\")\n",
    "plt.title(\"RMSE vs Forecast Horizon\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.25)\n",
    "plt.savefig(os.path.join(PLOTS_DIR, \"rmse_vs_horizon.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 3) For each horizon: Actual vs Predicted scatter for top models (Naive, Linear, RandomForest)\n",
    "# We use per_hour_errors to reconstruct preds as actual - error. Need y_test for actuals: reconstruct from errors keys (they store only errors arrays)\n",
    "# The code assumes all per_hour_errors[(h,model)] have same length and same ordering for a given horizon.\n",
    "# We'll choose first horizon to demonstrate and do Residual histogram/time series for all.\n",
    "for h in sorted({k[0] for k in per_hour_errors.keys()}):\n",
    "    h = int(h)\n",
    "    print(f\"\\nProcessing horizon {h}h\")\n",
    "    # gather models available for this horizon\n",
    "    models = [k[1] for k in per_hour_errors.keys() if k[0] == h]\n",
    "    # reconstruct actuals using Naive: y = naive_error + naive_pred but we don't have naive_pred directly\n",
    "    # Instead we can compute relative errors only (difference between models), and plot error distributions.\n",
    "    # We'll produce: residual histogram per model and paired test (Naive vs RF)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    for m in [\"Naive\", \"Linear\", \"RandomForest\", \"XGBoost\"]:\n",
    "        key = (h, m)\n",
    "        if key not in per_hour_errors: \n",
    "            continue\n",
    "        errs = np.array(per_hour_errors[key]).ravel()\n",
    "        plt.hist(errs, bins=80, alpha=0.5, density=False, label=f\"{m} (n={len(errs)})\")\n",
    "    plt.xlabel(\"Residual (actual - predicted)\")\n",
    "    plt.title(f\"Residual histogram (Horizon {h}h)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, f\"residual_hist_h{h}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # residual time-series sample (first 1000 points)\n",
    "    plt.figure(figsize=(9,3))\n",
    "    for m in [\"Naive\", \"RandomForest\", \"Linear\"]:\n",
    "        key = (h, m)\n",
    "        if key not in per_hour_errors: \n",
    "            continue\n",
    "        errs = np.array(per_hour_errors[key]).ravel()\n",
    "        plt.plot(errs[:1000], label=m, alpha=0.8)\n",
    "    plt.xlabel(\"Test index (sample)\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.title(f\"Residual time series (first 1000) - Horizon {h}h\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, f\"residual_ts_h{h}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Paired statistical tests: (Naive_error) - (RF_error) per hour => positive means RF smaller error\n",
    "    if (h, \"Naive\") in per_hour_errors and (h, \"RandomForest\") in per_hour_errors:\n",
    "        naive_err = np.array(per_hour_errors[(h, \"Naive\")]).ravel()\n",
    "        rf_err = np.array(per_hour_errors[(h, \"RandomForest\")]).ravel()\n",
    "        # Ensure same length\n",
    "        n = min(len(naive_err), len(rf_err))\n",
    "        naive_err, rf_err = naive_err[:n], rf_err[:n]\n",
    "        diff = naive_err - rf_err  # positive -> RF better\n",
    "        tstat, pval = ttest_rel(naive_err, rf_err)\n",
    "        # Wilcoxon (two-sided) - requires non-zero length and matched pairs\n",
    "        try:\n",
    "            wstat, wp = wilcoxon(naive_err - rf_err)\n",
    "        except Exception as e:\n",
    "            wstat, wp = np.nan, np.nan\n",
    "\n",
    "        print(f\"H{h}h paired t-test Naive vs RF: t={tstat:.3f}, p={pval:.3e}; Wilcoxon: stat={wstat}, p={wp:.3e}\")\n",
    "\n",
    "# 4) Feature importance (RandomForest) - load saved RF models if present\n",
    "import joblib, glob\n",
    "rf_files = glob.glob(os.path.join(RESULT_DIR, \"RandomForest_h*.joblib\"))\n",
    "if rf_files:\n",
    "    for f in rf_files:\n",
    "        # extract horizon from filename\n",
    "        basename = os.path.basename(f)\n",
    "        h = int(basename.split(\"_h\")[-1].split(\".joblib\")[0])\n",
    "        model = joblib.load(f)\n",
    "        if hasattr(model, \"feature_importances_\"):\n",
    "            # we need the feature names saved in rmse run: we will rebuild X for that horizon to get column names\n",
    "            # load df again (assume same DATA_PATH used earlier)\n",
    "            df = pd.read_csv(\"PRSA_Aotizhongxin_cleaned.csv\", parse_dates=True, index_col=0)\n",
    "            # create supervised to get X columns\n",
    "            def make_supervised_local(df, target_col, lags=3, horizon=1):\n",
    "                df_sup = df.copy()\n",
    "                for lag in range(1, lags+1):\n",
    "                    df_sup = df_sup.assign(**{f\"{c}_lag{lag}\": df[c].shift(lag) for c in df.columns})\n",
    "                df_sup[f\"y_h{horizon}\"] = df_sup[target_col].shift(-horizon)\n",
    "                df_sup = df_sup.dropna()\n",
    "                X = df_sup.drop(columns=[f\"y_h{horizon}\"])\n",
    "                return X\n",
    "            X = make_supervised_local(df, \"PM2.5\", lags=3, horizon=h)\n",
    "            # convert object cols same as training\n",
    "            obj_cols = X.select_dtypes(include=['object']).columns\n",
    "            for c in obj_cols:\n",
    "                X[c] = pd.to_numeric(X[c], errors='coerce').fillna(0)\n",
    "            # now get importance\n",
    "            fi = model.feature_importances_\n",
    "            idx = np.argsort(fi)[-30:]  # top 30\n",
    "            top_feats = X.columns[idx]\n",
    "            plt.figure(figsize=(6,8))\n",
    "            plt.barh(range(len(idx)), fi[idx])\n",
    "            plt.yticks(range(len(idx)), top_feats)\n",
    "            plt.xlabel(\"Importance\")\n",
    "            plt.title(f\"RandomForest top features (H{h}h)\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(PLOTS_DIR, f\"rf_feature_importance_h{h}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "print(\"\\nAll plots saved to:\", PLOTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f16ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Tuning for horizon 1 hour(s)\n",
      "Previous RMSEs (approx): Linear=17.4797, RF=18.1745, XGB=19.3884, Naive=30.6176\n",
      "Running RandomForest RandomizedSearchCV (single-process)...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "9 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    call to `partial_fit`. All other methods that validate `X`\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    f\" {constraints_str}. Got {param_val!r} instead.\"\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  If a numeric value is given, FitFailedWarning is raised.\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [-501.47429851 -794.3555754  -428.21999612 -429.13088508 -797.07756161\n",
      " -435.69634185 -501.97167056 -420.65132658           nan -433.94010019\n",
      "           nan -435.92445383 -488.83021509           nan -846.68877059\n",
      " -450.35771734 -404.6986915  -405.43349822 -439.80923355 -455.93867884]\n",
      "  ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF tuned RMSE (H1): 17.5546\n",
      "Running XGBoost RandomizedSearchCV (single-process)...\n",
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 155\u001b[0m\n\u001b[1;32m    144\u001b[0m xgb_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m    145\u001b[0m     xg,\n\u001b[1;32m    146\u001b[0m     xgb_param_dist,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning XGBoost RandomizedSearchCV (single-process)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m \u001b[43mxgb_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m best_xgb \u001b[38;5;241m=\u001b[39m xgb_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m    157\u001b[0m xgb_pred \u001b[38;5;241m=\u001b[39m best_xgb\u001b[38;5;241m.\u001b[39mpredict(X_test_num)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1011\u001b[0m _store(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;66;03m# Use one MaskedArray and mask all the places where the param is not\u001b[39;00m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;66;03m# applicable for that candidate. Use defaultdict as each candidate may\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# not contain all the params\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m param_results \u001b[38;5;241m=\u001b[39m defaultdict(\n\u001b[1;32m   1016\u001b[0m     partial(\n\u001b[1;32m   1017\u001b[0m         MaskedArray,\n\u001b[1;32m   1018\u001b[0m         np\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m   1019\u001b[0m             n_candidates,\n\u001b[1;32m   1020\u001b[0m         ),\n\u001b[1;32m   1021\u001b[0m         mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1022\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m,\n\u001b[1;32m   1023\u001b[0m     )\n\u001b[0;32m-> 1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cand_idx, params \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(candidate_params):\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;66;03m# An all masked empty array gets created for the key\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;66;03m# `\"param_%s\" % name` at the first occurrence of `name`.\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m         \u001b[38;5;66;03m# Setting the value at an index also unmasks that index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1951\u001b[0m, in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    967\u001b[0m array = np.array(array, dtype=np.float64).reshape(n_candidates, n_splits)\n\u001b[1;32m    968\u001b[0m if splits:\n\u001b[1;32m    969\u001b[0m     for split_idx in range(n_splits):\n\u001b[0;32m--> 970\u001b[0m         # Uses closure to alter the results\n\u001b[1;32m    971\u001b[0m         results[\"split%d_%s\" % (split_idx, key_name)] = array[:, split_idx]\n\u001b[1;32m    973\u001b[0m array_means = np.average(array, axis=1, weights=weights)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelayed\u001b[39m(function):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decorator used to capture the arguments of a function.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    This alternative to `joblib.delayed` is meant to be used in conjunction\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    with `sklearn.utils.parallel.Parallel`. The latter captures the the scikit-\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    learn configuration by calling `sklearn.get_config()` in the current\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    thread, prior to dispatching the first task. The captured configuration is\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    then propagated and enabled for the duration of the execution of the\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;124;03m    delayed function in the joblib workers.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 1.3\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m       `delayed` was moved from `sklearn.utils.fixes` to `sklearn.utils.parallel`\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m       in scikit-learn 1.3.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    function : callable\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m        The function to be delayed.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    output: tuple\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m        Tuple containing the delayed function, the positional arguments, and the\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m        keyword arguments.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(function)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelayed_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FuncWrapper(function), args, kwargs\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/joblib/parallel.py:1986\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/joblib/parallel.py:1914\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1914\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:866\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/xgboost/core.py:750\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    749\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/xgboost/sklearn.py:1368\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1366\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/xgboost/core.py:750\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    749\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/xgboost/training.py:199\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/xgboost/core.py:2410\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2409\u001b[0m     _check_call(\n\u001b[0;32m-> 2410\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m     )\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2415\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tune_models_fixed.py\n",
    "# Hyperparameter tuning for RandomForest and XGBoost across multiple horizons.\n",
    "# This variant forces single-process CV to avoid joblib serialization errors.\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "DATA_PATH = \"PRSA_Aotizhongxin_cleaned.csv\"\n",
    "TARGET = \"PM2.5\"\n",
    "LAGS = 3\n",
    "HORIZONS = [1, 3, 6, 12, 24]\n",
    "TRAIN_FRAC = 0.8\n",
    "RESULT_DIR = \"results_multi_horizon\"\n",
    "TUNE_DIR = os.path.join(RESULT_DIR, \"tuned_models\")\n",
    "os.makedirs(TUNE_DIR, exist_ok=True)\n",
    "RANDOM_STATE = 42\n",
    "RF_ITER = 20     # reduce iterations to keep total time reasonable\n",
    "XGB_ITER = 25\n",
    "N_JOBS_SEARCH = 1   # IMPORTANT: run CV single-threaded to avoid pickling issues\n",
    "N_JOBS_EST = 1      # estimator-level parallelism inside CV (keep 1 to avoid nested parallelism)\n",
    "N_SPLITS = 3\n",
    "# ----------------------------\n",
    "\n",
    "def make_supervised(df, target_col, lags=3, horizon=1):\n",
    "    df_sup = df.copy()\n",
    "    for lag in range(1, lags + 1):\n",
    "        df_sup = df_sup.assign(**{f\"{c}_lag{lag}\": df[c].shift(lag) for c in df.columns})\n",
    "    df_sup[f\"y_h{horizon}\"] = df_sup[target_col].shift(-horizon)\n",
    "    df_sup = df_sup.dropna()\n",
    "    X = df_sup.drop(columns=[f\"y_h{horizon}\"])\n",
    "    y = df_sup[f\"y_h{horizon}\"]\n",
    "    return X, y\n",
    "\n",
    "def train_test_split_time(X, y, train_frac=0.8):\n",
    "    n = len(X)\n",
    "    split = int(n * train_frac)\n",
    "    return X.iloc[:split], X.iloc[split:], y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "def ensure_numeric(X_train, X_test):\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "    obj_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    for c in obj_cols:\n",
    "        X_train[c] = pd.to_numeric(X_train[c], errors='coerce')\n",
    "        X_test[c] = pd.to_numeric(X_test[c], errors='coerce')\n",
    "        if X_train[c].dtype == 'object' or X_test[c].dtype == 'object':\n",
    "            X_train[c] = X_train[c].map({'True':1, 'False':0, True:1, False:0}).astype('float')\n",
    "            X_test[c]  = X_test[c].map({'True':1, 'False':0, True:1, False:0}).astype('float')\n",
    "    X_train = X_train.fillna(0).astype(float)\n",
    "    X_test  = X_test.fillna(0).astype(float)\n",
    "    return X_train, X_test\n",
    "\n",
    "# load df\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=True, index_col=0)\n",
    "\n",
    "# Try to load previous rmse table (optional)\n",
    "prev_rmse_path = os.path.join(RESULT_DIR, \"rmse_table.csv\")\n",
    "prev_rmse = pd.read_csv(prev_rmse_path, index_col=0) if os.path.exists(prev_rmse_path) else None\n",
    "\n",
    "tuning_summary = []\n",
    "\n",
    "for h in HORIZONS:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Tuning for horizon {h} hour(s)\")\n",
    "    X, y = make_supervised(df, TARGET, lags=LAGS, horizon=h)\n",
    "    X_train, X_test, y_train, y_test = train_test_split_time(X, y, TRAIN_FRAC)\n",
    "    X_train_num, X_test_num = ensure_numeric(X_train, X_test)\n",
    "\n",
    "    # get previous RMSEs if available, else compute defaults quickly\n",
    "    if prev_rmse is not None:\n",
    "        try:\n",
    "            prev_lin = float(prev_rmse.loc[\"Linear\", str(h)])\n",
    "            prev_rf  = float(prev_rmse.loc[\"RandomForest\", str(h)])\n",
    "            prev_xgb = float(prev_rmse.loc[\"XGBoost\", str(h)])\n",
    "            prev_naive = float(prev_rmse.loc[\"Naive\", str(h)])\n",
    "        except Exception:\n",
    "            prev_lin = prev_rf = prev_xgb = prev_naive = np.nan\n",
    "    else:\n",
    "        # compute naive + defaults\n",
    "        naive_pred = X_test_num[f\"{TARGET}_lag1\"].values\n",
    "        prev_naive = mean_squared_error(y_test, naive_pred)**0.5\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        lin = make_pipeline(StandardScaler(), LinearRegression())\n",
    "        lin.fit(X_train_num, y_train)\n",
    "        prev_lin = mean_squared_error(y_test, lin.predict(X_test_num))**0.5\n",
    "        rf_def = RandomForestRegressor(n_estimators=200, random_state=RANDOM_STATE, n_jobs=1)\n",
    "        rf_def.fit(X_train_num, y_train)\n",
    "        prev_rf = mean_squared_error(y_test, rf_def.predict(X_test_num))**0.5\n",
    "        xgb_def = xgb.XGBRegressor(n_estimators=200, random_state=RANDOM_STATE, n_jobs=1, verbosity=0, objective='reg:squarederror')\n",
    "        xgb_def.fit(X_train_num, y_train)\n",
    "        prev_xgb = mean_squared_error(y_test, xgb_def.predict(X_test_num))**0.5\n",
    "\n",
    "    print(f\"Previous RMSEs (approx): Linear={prev_lin:.4f}, RF={prev_rf:.4f}, XGB={prev_xgb:.4f}, Naive={prev_naive:.4f}\")\n",
    "\n",
    "    # ----------------- Random Forest Tuning -----------------\n",
    "    rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS_EST)\n",
    "    rf_param_dist = {\n",
    "        \"n_estimators\": randint(100, 600),\n",
    "        \"max_depth\": randint(4, 30),\n",
    "        \"min_samples_split\": randint(2, 20),\n",
    "        \"min_samples_leaf\": randint(1, 20),\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\", 0.3, 0.5, 0.8]\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        rf,\n",
    "        rf_param_dist,\n",
    "        n_iter=RF_ITER,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=tscv,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS_SEARCH,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Running RandomForest RandomizedSearchCV (single-process)...\")\n",
    "    rf_search.fit(X_train_num, y_train)\n",
    "    best_rf = rf_search.best_estimator_\n",
    "    rf_pred = best_rf.predict(X_test_num)\n",
    "    rf_rmse_tuned = mean_squared_error(y_test, rf_pred)**0.5\n",
    "    print(f\"RF tuned RMSE (H{h}): {rf_rmse_tuned:.4f}\")\n",
    "    joblib.dump(best_rf, os.path.join(TUNE_DIR, f\"RandomForest_h{h}_tuned.joblib\"))\n",
    "\n",
    "    # ----------------- XGBoost Tuning -----------------\n",
    "    xg = xgb.XGBRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS_EST, verbosity=0, objective='reg:squarederror')\n",
    "    xgb_param_dist = {\n",
    "        \"n_estimators\": randint(100, 800),\n",
    "        \"max_depth\": randint(2, 12),\n",
    "        \"learning_rate\": uniform(0.01, 0.4),\n",
    "        \"subsample\": uniform(0.5, 0.5),\n",
    "        \"colsample_bytree\": uniform(0.4, 0.6),\n",
    "        \"gamma\": uniform(0, 5),\n",
    "        \"reg_alpha\": uniform(0, 5),\n",
    "        \"reg_lambda\": uniform(0, 5)\n",
    "    }\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        xg,\n",
    "        xgb_param_dist,\n",
    "        n_iter=XGB_ITER,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=tscv,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=N_JOBS_SEARCH,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Running XGBoost RandomizedSearchCV (single-process)...\")\n",
    "    xgb_search.fit(X_train_num, y_train)\n",
    "    best_xgb = xgb_search.best_estimator_\n",
    "    xgb_pred = best_xgb.predict(X_test_num)\n",
    "    xgb_rmse_tuned = mean_squared_error(y_test, xgb_pred)**0.5\n",
    "    print(f\"XGB tuned RMSE (H{h}): {xgb_rmse_tuned:.4f}\")\n",
    "    joblib.dump(best_xgb, os.path.join(TUNE_DIR, f\"XGBoost_h{h}_tuned.joblib\"))\n",
    "\n",
    "    # record\n",
    "    tuning_summary.append({\n",
    "        \"horizon\": h,\n",
    "        \"prev_linear\": prev_lin,\n",
    "        \"prev_rf\": prev_rf,\n",
    "        \"prev_xgb\": prev_xgb,\n",
    "        \"prev_naive\": prev_naive,\n",
    "        \"rf_tuned_rmse\": float(rf_rmse_tuned),\n",
    "        \"xgb_tuned_rmse\": float(xgb_rmse_tuned),\n",
    "        \"rf_best_params\": rf_search.best_params_,\n",
    "        \"xgb_best_params\": xgb_search.best_params_\n",
    "    })\n",
    "\n",
    "# save tuning summary and build RMSE table\n",
    "joblib.dump(tuning_summary, os.path.join(RESULT_DIR, \"tuning_summary_fixed.joblib\"))\n",
    "\n",
    "# build final pivot table\n",
    "rows = []\n",
    "for t in tuning_summary:\n",
    "    h = str(t[\"horizon\"])\n",
    "    rows.append((\"Linear\", h, t[\"prev_linear\"]))\n",
    "    rows.append((\"Naive\", h, t[\"prev_naive\"]))\n",
    "    rows.append((\"RandomForest\", h, t[\"rf_tuned_rmse\"]))\n",
    "    rows.append((\"XGBoost\", h, t[\"xgb_tuned_rmse\"]))\n",
    "summary_df = pd.DataFrame(rows, columns=[\"model\", \"horizon\", \"rmse\"])\n",
    "pivot = summary_df.pivot_table(index=\"model\", columns=\"horizon\", values=\"rmse\", aggfunc='first')\n",
    "pivot = pivot.reindex(index=[\"Linear\", \"Naive\", \"RandomForest\", \"XGBoost\"])\n",
    "pivot.to_csv(os.path.join(RESULT_DIR, \"rmse_table_tuned_fixed.csv\"))\n",
    "\n",
    "print(\"\\nTuning complete. Tuned models in:\", TUNE_DIR)\n",
    "print(\"Tuned RMSE table saved to:\", os.path.join(RESULT_DIR, \"rmse_table_tuned_fixed.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74fcafdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "FAST tuning for horizon 1h\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      " RF fast RMSE H1: 17.6423\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      " XGB fast RMSE H1: 17.6682\n",
      "\n",
      "========================================\n",
      "FAST tuning for horizon 3h\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      " RF fast RMSE H3: 36.0534\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      " XGB fast RMSE H3: 37.1736\n",
      "\n",
      "========================================\n",
      "FAST tuning for horizon 6h\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      " RF fast RMSE H6: 50.6169\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      " XGB fast RMSE H6: 51.0720\n",
      "\n",
      "========================================\n",
      "FAST tuning for horizon 12h\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      " RF fast RMSE H12: 63.8022\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      " XGB fast RMSE H12: 63.8262\n",
      "\n",
      "========================================\n",
      "FAST tuning for horizon 24h\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      " RF fast RMSE H24: 71.9067\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      " XGB fast RMSE H24: 71.4334\n",
      "\n",
      "FAST tuning done. Results in: results_multi_horizon/tuned_models_fast\n",
      "Summary: results_multi_horizon/rmse_table_tuned_fast.csv\n"
     ]
    }
   ],
   "source": [
    "# tune_models_fast.py  (fast, laptop-friendly)\n",
    "import os, joblib, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# CONFIG (fast)\n",
    "DATA_PATH = \"PRSA_Aotizhongxin_cleaned.csv\"\n",
    "TARGET = \"PM2.5\"\n",
    "LAGS = 3\n",
    "HORIZONS = [1, 3, 6, 12, 24]\n",
    "TRAIN_FRAC = 0.8\n",
    "RESULT_DIR = \"results_multi_horizon\"\n",
    "TUNE_DIR = os.path.join(RESULT_DIR, \"tuned_models_fast\")\n",
    "os.makedirs(TUNE_DIR, exist_ok=True)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# FAST: reduce iterations & folds\n",
    "RF_ITER = 8\n",
    "XGB_ITER = 10\n",
    "N_JOBS_SEARCH = 1      # safe single-process CV\n",
    "N_JOBS_EST = 1\n",
    "N_SPLITS = 2\n",
    "\n",
    "def make_supervised(df, target_col, lags=3, horizon=1):\n",
    "    df_sup = df.copy()\n",
    "    for lag in range(1, lags + 1):\n",
    "        df_sup = df_sup.assign(**{f\"{c}_lag{lag}\": df[c].shift(lag) for c in df.columns})\n",
    "    df_sup[f\"y_h{horizon}\"] = df_sup[target_col].shift(-horizon)\n",
    "    df_sup = df_sup.dropna()\n",
    "    X = df_sup.drop(columns=[f\"y_h{horizon}\"])\n",
    "    y = df_sup[f\"y_h{horizon}\"]\n",
    "    return X, y\n",
    "\n",
    "def train_test_split_time(X, y, train_frac=0.8):\n",
    "    n = len(X)\n",
    "    split = int(n * train_frac)\n",
    "    return X.iloc[:split], X.iloc[split:], y.iloc[:split], y.iloc[split:]\n",
    "\n",
    "def ensure_numeric(X_train, X_test):\n",
    "    X_train = X_train.copy(); X_test = X_test.copy()\n",
    "    obj_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    for c in obj_cols:\n",
    "        X_train[c] = pd.to_numeric(X_train[c], errors='coerce')\n",
    "        X_test[c]  = pd.to_numeric(X_test[c], errors='coerce')\n",
    "        if X_train[c].dtype == 'object' or X_test[c].dtype == 'object':\n",
    "            X_train[c] = X_train[c].map({'True':1,'False':0,True:1,False:0}).astype(float)\n",
    "            X_test[c]  = X_test[c].map({'True':1,'False':0,True:1,False:0}).astype(float)\n",
    "    X_train = X_train.fillna(0).astype(float)\n",
    "    X_test  = X_test.fillna(0).astype(float)\n",
    "    return X_train, X_test\n",
    "\n",
    "# load df\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=True, index_col=0)\n",
    "\n",
    "tuning_summary = []\n",
    "\n",
    "for h in HORIZONS:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"FAST tuning for horizon {h}h\")\n",
    "    X, y = make_supervised(df, TARGET, lags=LAGS, horizon=h)\n",
    "    X_train, X_test, y_train, y_test = train_test_split_time(X, y, TRAIN_FRAC)\n",
    "    X_train_num, X_test_num = ensure_numeric(X_train, X_test)\n",
    "\n",
    "    # quick previous RMSEs using defaults (so we have a \"before\")\n",
    "    naive_pred = X_test_num[f\"{TARGET}_lag1\"].values\n",
    "    prev_naive = mean_squared_error(y_test, naive_pred)**0.5\n",
    "\n",
    "    # ---------- RF fast search ----------\n",
    "    rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS_EST)\n",
    "    rf_param_dist = {\n",
    "        \"n_estimators\": randint(100, 400),\n",
    "        \"max_depth\": randint(5, 25),\n",
    "        \"min_samples_split\": randint(2, 10),\n",
    "        \"min_samples_leaf\": randint(1, 6),\n",
    "        # REMOVE 'auto' — use allowed options only\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None, 0.3, 0.6]\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "    rf_search = RandomizedSearchCV(\n",
    "        rf, rf_param_dist, n_iter=RF_ITER,\n",
    "        scoring=\"neg_mean_squared_error\", cv=tscv,\n",
    "        random_state=RANDOM_STATE, n_jobs=N_JOBS_SEARCH, verbose=1\n",
    "    )\n",
    "    rf_search.fit(X_train_num, y_train)\n",
    "    best_rf = rf_search.best_estimator_\n",
    "    rf_pred = best_rf.predict(X_test_num)\n",
    "    rf_rmse = mean_squared_error(y_test, rf_pred)**0.5\n",
    "    joblib.dump(best_rf, os.path.join(TUNE_DIR, f\"RandomForest_h{h}_fast.joblib\"))\n",
    "    print(f\" RF fast RMSE H{h}: {rf_rmse:.4f}\")\n",
    "\n",
    "    # ---------- XGB fast search ----------\n",
    "    xg = xgb.XGBRegressor(random_state=RANDOM_STATE, n_jobs=N_JOBS_EST, verbosity=0, objective='reg:squarederror')\n",
    "    xgb_param_dist = {\n",
    "        \"n_estimators\": randint(100, 400),\n",
    "        \"max_depth\": randint(3, 8),\n",
    "        \"learning_rate\": uniform(0.02, 0.3),\n",
    "        \"subsample\": uniform(0.6, 0.4),\n",
    "        \"colsample_bytree\": uniform(0.5, 0.4),\n",
    "        \"gamma\": uniform(0, 2)\n",
    "    }\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        xg, xgb_param_dist, n_iter=XGB_ITER,\n",
    "        scoring=\"neg_mean_squared_error\", cv=tscv,\n",
    "        random_state=RANDOM_STATE, n_jobs=N_JOBS_SEARCH, verbose=1\n",
    "    )\n",
    "    xgb_search.fit(X_train_num, y_train)\n",
    "    best_xgb = xgb_search.best_estimator_\n",
    "    xgb_pred = best_xgb.predict(X_test_num)\n",
    "    xgb_rmse = mean_squared_error(y_test, xgb_pred)**0.5\n",
    "    joblib.dump(best_xgb, os.path.join(TUNE_DIR, f\"XGBoost_h{h}_fast.joblib\"))\n",
    "    print(f\" XGB fast RMSE H{h}: {xgb_rmse:.4f}\")\n",
    "\n",
    "    tuning_summary.append({\n",
    "        \"horizon\": h,\n",
    "        \"prev_naive\": float(prev_naive),\n",
    "        \"rf_fast_rmse\": float(rf_rmse),\n",
    "        \"xgb_fast_rmse\": float(xgb_rmse),\n",
    "        \"rf_best_params\": rf_search.best_params_,\n",
    "        \"xgb_best_params\": xgb_search.best_params_\n",
    "    })\n",
    "\n",
    "# save results\n",
    "joblib.dump(tuning_summary, os.path.join(RESULT_DIR, \"tuning_summary_fast.joblib\"))\n",
    "\n",
    "rows = []\n",
    "for t in tuning_summary:\n",
    "    h = str(t[\"horizon\"])\n",
    "    rows.append((\"Naive\", h, t[\"prev_naive\"]))\n",
    "    rows.append((\"RandomForest\", h, t[\"rf_fast_rmse\"]))\n",
    "    rows.append((\"XGBoost\", h, t[\"xgb_fast_rmse\"]))\n",
    "summary_df = pd.DataFrame(rows, columns=[\"model\",\"horizon\",\"rmse\"])\n",
    "pivot = summary_df.pivot_table(index=\"model\", columns=\"horizon\", values=\"rmse\", aggfunc='first')\n",
    "pivot = pivot.reindex(index=[\"Naive\",\"RandomForest\",\"XGBoost\"])\n",
    "pivot.to_csv(os.path.join(RESULT_DIR, \"rmse_table_tuned_fast.csv\"))\n",
    "print(\"\\nFAST tuning done. Results in:\", TUNE_DIR)\n",
    "print(\"Summary:\", os.path.join(RESULT_DIR, \"rmse_table_tuned_fast.csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
